{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96259b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from os.path import expanduser\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
    "sys.path.append(ROOT_DIR)\n",
    "home = expanduser(\"~\")\n",
    "sys.path.append(os.path.abspath(home+'/AI-SDC'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "import tensorflow_privacy as tf_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "# Classifiers for attack models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Safe Keras\n",
    "from safemodel.classifiers import SafeKerasModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0599f07",
   "metadata": {},
   "source": [
    "## A Quick Start Guide to implementing Safer Keras Models\n",
    "### Definition of the datasets\n",
    "1. We draw data points from a distribution.\n",
    "2. We split these data points into the target dataset and a shadow dataset drawn from the same distribution.\n",
    "3. We also draw a dataset from a different distribution.\n",
    "\n",
    "**NOTE**. ***we make datasets with few samples but with many features to force the target model to overfit.***\n",
    "\n",
    "\n",
    "**NOTE**: batch_size 25 so DP optimizer would run with same hyperparams\n",
    "\n",
    "**NOTE**: Next cell detemrienswhich dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bde25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data_for_pytests = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not simple_data_for_pytests:\n",
    "    n_classes = 2\n",
    "\n",
    "    # (X,y): Original distribution\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_classes=n_classes,\n",
    "        n_features=300,\n",
    "        n_informative=300,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        random_state=15,\n",
    "    )\n",
    "    # One-hot encoding of the label\n",
    "    y = np.eye(n_classes)[y]\n",
    "\n",
    "    # (Xt, yt) is the target dataset, owned by the TRE and drawn from the (X,y) distribution\n",
    "    # (Xs, ys) is a shadow dataset drawn from the (X,y) distribution\n",
    "    Xt, Xs, yt, ys = train_test_split(X, y, test_size=0.50, random_state=15)\n",
    "\n",
    "    # (Xd, yd) is a shadow dataset, drawn from a different distribution (different seed)\n",
    "    Xd, yd = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_classes=n_classes,\n",
    "        n_features=300,\n",
    "        n_informative=300,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        random_state=42,\n",
    "    )\n",
    "    yd = np.eye(n_classes)[yd]\n",
    "\n",
    "    # Split into train (member) and test (non-member) datasets\n",
    "    # Set shuffle to False so that Xt_membership is consistent with Xt, otherwise\n",
    "    # we need to stack Xt_member and Xt_nonmember again to get a consistent Xt.\n",
    "    Xt_member, Xt_nonmember, yt_member, yt_nonmember = train_test_split(\n",
    "        Xt, yt, test_size=0.5, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Set membership status for future tests\n",
    "    Xt_membership = np.vstack(\n",
    "        (\n",
    "            np.ones((Xt_member.shape[0], 1), np.uint8),\n",
    "            np.zeros((Xt_nonmember.shape[0], 1), np.uint8),\n",
    "        )\n",
    "    ).flatten()\n",
    "\n",
    "    X = Xt_member\n",
    "    y = yt_member\n",
    "    Xval = Xt_nonmember\n",
    "    yval = yt_nonmember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8845b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simple_data_for_pytests:\n",
    "    from sklearn import datasets\n",
    "\n",
    "    def get_data():\n",
    "        iris = datasets.load_iris()\n",
    "        x = np.asarray(iris.data, dtype=np.float64)\n",
    "        y = np.asarray(iris.target, dtype=np.float64)\n",
    "        x = np.vstack([x, (7, 2.0, 4.5, 1)])\n",
    "        y = np.append(y, 4)\n",
    "        return x, y\n",
    "\n",
    "    xall, yall = get_data()\n",
    "    n_classes = 4\n",
    "    X, Xval, y, yval = train_test_split(\n",
    "        xall, yall, test_size=0.2, shuffle=True, random_state=12345\n",
    "    )\n",
    "\n",
    "    y = tf.one_hot(y, n_classes)\n",
    "    yval = tf.one_hot(yval, n_classes)\n",
    "# yval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b7816",
   "metadata": {},
   "source": [
    "## Define the target model architecture\n",
    "\n",
    "*Again, we use a rather big model (for the classification task) to favour overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20787df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target model\n",
    "# Tensorflow model (MLP) (making it big to make it overfit)\n",
    "\n",
    "# amke results repeatable\n",
    "tf.random.set_seed(12345)\n",
    "initializer = tf.keras.initializers.Zeros()\n",
    "\n",
    "input_data = Input(shape=X[0].shape)\n",
    "x = Dense(128, activation=\"relu\", kernel_initializer=initializer)(input_data)\n",
    "x = Dense(128, activation=\"relu\", kernel_initializer=initializer)(x)\n",
    "x = Dense(64, activation=\"relu\", kernel_initializer=initializer)(x)\n",
    "output = Dense(n_classes, activation=\"softmax\", kernel_initializer=initializer)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbed581",
   "metadata": {},
   "source": [
    "### Define the SafeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944288d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel = SafeKerasModel(\n",
    "    inputs=input_data,\n",
    "    outputs=output,\n",
    "    name=\"safekeras-test\",\n",
    "    num_samples=X.shape[0],\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534c8b7",
   "metadata": {},
   "source": [
    "### Set loss and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "\n",
    "safeModel.compile(loss=loss, optimizer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4a371",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "r_DP = safeModel.fit(\n",
    "    X,\n",
    "    y,\n",
    "    validation_data=(Xval, yval),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "if r_DP == None:\n",
    "    print(\"You have chosen to exit. Reset relevant parameter values then re-run fit().\")\n",
    "else:\n",
    "    plt.plot(r_DP.history[\"accuracy\"], label=\"accuracy\")\n",
    "    plt.plot(r_DP.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    loss, acc = safeModel.evaluate(X, y)\n",
    "    print(f\"training loss {loss} accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059f431",
   "metadata": {},
   "source": [
    "### Compute privacy and check if requirements for Differential Privacy are met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55805bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = X.shape[0]\n",
    "batch_size = safeModel.batch_size\n",
    "epochs = 20\n",
    "\n",
    "dp_met, privacy = safeModel.dp_epsilon_met(num_examples=num_samples, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "print(f\"with these settings privacy = {privacy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88413967",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_met, msg = safeModel.check_epsilon(X.shape[0], safeModel.batch_size, safeModel.epochs)\n",
    "print(f'Satisfies DP: {dp_met}')\n",
    "print(f'{msg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21813930",
   "metadata": {},
   "source": [
    "### Check model and request release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "safeModel.save(\"safe1.h5\")\n",
    "safeModel.preliminary_check()\n",
    "safeModel.request_release(filename=\"safe1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e46a5",
   "metadata": {},
   "source": [
    "### Examine Checkfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"contents of checkfile are\"; cat *_check*ile.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb08fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f3a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
